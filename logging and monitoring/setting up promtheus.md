Step 1: Install Prometheus Using Helm
Helm is a package manager for Kubernetes that simplifies the deployment of applications. The Prometheus Helm chart is widely used to deploy Prometheus in a Kubernetes cluster.

Add the Prometheus Helm repository:

sh
```
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```
Install Prometheus using Helm:

sh
```
helm install prometheus prometheus-community/prometheus
```
This command installs Prometheus with default configurations. You can customize the installation by modifying the Helm values.

Verify the installation:

sh
```
kubectl get pods -l app=prometheus
```
Ensure that the Prometheus server and its components are running correctly.

Step 2: Configure Service Discovery
Prometheus will automatically discover Kubernetes services and pods based on annotations or labels. However, you can configure specific scraping rules by editing the prometheus.yml configuration file.

Example configuration for scraping Kubernetes nodes:
yaml
```
scrape_configs:
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
```
Step 3: Expose Prometheus
To access the Prometheus UI, you need to expose the Prometheus service.

Expose Prometheus using a LoadBalancer or NodePort:

sh
```
kubectl expose deployment prometheus-server --type=NodePort --name=prometheus-service
```
Or, for a LoadBalancer:

sh
```
kubectl expose deployment prometheus-server --type=LoadBalancer --name=prometheus-service
```
Access Prometheus UI:

Once exposed, you can access the Prometheus UI through the external IP or NodePort. The UI allows you to run PromQL queries, visualize metrics, and monitor the health of your targets.
Step 4: Set Up Alerting
Prometheus includes a basic alerting mechanism that evaluates rules and sends notifications when conditions are met.

Define alerting rules:

Create a file named alerts.yml and define your alerting rules. For example:
yaml
```
groups:
- name: example
  rules:
  - alert: HighCPUUsage
    expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (pod) > 0.8
    for: 1m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage detected"
      description: "CPU usage is above 80% for pod {{ $labels.pod }}"
```
Configure Alertmanager:

Set up Alertmanager to handle and route alerts generated by Prometheus. Alertmanager can send alerts to various channels like email, Slack, or PagerDuty.
Step 5: Visualize Metrics with Grafana
Grafana is a popular open-source platform for monitoring and observability, which integrates well with Prometheus.

Install Grafana using Helm:

sh
```
helm install grafana grafana/grafana
```
Connect Prometheus to Grafana:

In Grafana, add Prometheus as a data source. Use the Prometheus service URL (http://prometheus-service:9090) as the data source URL.
Create dashboards:

Create Grafana dashboards to visualize the metrics collected by Prometheus. Grafana offers pre-built dashboards for Kubernetes, which you can import and customize.
Autoscaling with Prometheus
Prometheus metrics can be used for more advanced autoscaling scenarios beyond basic CPU and memory utilization.

Install the Prometheus Adapter:

The Prometheus Adapter allows Kubernetes to use custom Prometheus metrics for autoscaling.
sh
```
helm install prometheus-adapter prometheus-community/prometheus-adapter
```
Create a custom metric:

Define a custom metric in Prometheus (e.g., request latency) and configure the Prometheus Adapter to expose it to the Kubernetes API.
Use custom metrics in HPA:

Configure an HPA to scale based on the custom Prometheus metric.
yaml
```
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: custom-metric-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Pods
    pods:
      metricName: custom_metric_name
      target:
        type: AverageValue
        averageValue: 100m
```